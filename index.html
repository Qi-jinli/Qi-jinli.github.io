<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/logo.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/logo.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo.jpeg">
  <link rel="mask-icon" href="/images/logo.jpeg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"qi-jinli.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="毕业于大连理工大学机械系，现从事人工智能开发，主要从事机器学习，计算机视觉，自然语言处理等多项领域。">
<meta property="og:type" content="website">
<meta property="og:title" content="畅快的伊瓦西">
<meta property="og:url" content="http://qi-jinli.github.io/index.html">
<meta property="og:site_name" content="畅快的伊瓦西">
<meta property="og:description" content="毕业于大连理工大学机械系，现从事人工智能开发，主要从事机器学习，计算机视觉，自然语言处理等多项领域。">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Qijinli">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://qi-jinli.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>畅快的伊瓦西</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">畅快的伊瓦西</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">大直若屈，大巧若拙，大辩若讷，大赢若纳。</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section">首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section">关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section">标签<span class="badge">27</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section">分类<span class="badge">5</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section">归档<span class="badge">22</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger">搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://qi-jinli.github.io/2021/05/05/HMM%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.jpeg">
      <meta itemprop="name" content="Qijinli">
      <meta itemprop="description" content="毕业于大连理工大学机械系，现从事人工智能开发，主要从事机器学习，计算机视觉，自然语言处理等多项领域。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="畅快的伊瓦西">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/05/HMM%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">HMM模型</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-05 19:05:19" itemprop="dateCreated datePublished" datetime="2021-05-05T19:05:19+08:00">2021-05-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-11-19 16:55:44" itemprop="dateModified" datetime="2021-11-19T16:55:44+08:00">2021-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h1><blockquote>
<p><code>马尔科夫链</code>即为状态空间中从一个状态到另一个状态转换的随机过程。</p>
<p>该过程具备<code>无记忆</code>的性质: 下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。这种特定类型的“无记忆性”称作马尔可夫性质。</p>
<p>马尔可夫链的数学表示为:</p>
<p><img src="/2021/05/05/HMM%E6%A8%A1%E5%9E%8B/image-20210304182713593.png" alt="image-20210304182713593" style="zoom:75%;"></p>
</blockquote>
<h1 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h1><blockquote>
<p><code>隐马尔可夫模型(Hidden Markov Model，HMM)</code>是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。</p>
<p>什么样的问题解决可以用HMM模型?</p>
<ol>
<li>我们的问题是基于序列的，比如时间序列，或者状态序列。</li>
<li>我们的问题中有两类数据:<ol>
<li>一类序列数据是可以观测到的，即观测序列; </li>
<li>而另一类数据是不能观察到的，即隐藏状态序列，简称状态序列。</li>
</ol>
</li>
</ol>
<p><strong>模型定义：</strong></p>
<ol>
<li><p>假设 <em>Q</em> 是所有可能的隐藏状态的集合，<em>V</em> 是所有可能的观测状态的集合，即:</p>
<p><img src="/2021/05/05/HMM%E6%A8%A1%E5%9E%8B/image-20210304183145189.png" alt="image-20210304183145189" style="zoom:50%;"></p>
</li>
<li><p>对于一个长度为<em>T</em> 的序列，<em>i</em> 是对应的状态序列, <em>O</em> 是对应的观察序列，即:</p>
<p><img src="/2021/05/05/HMM%E6%A8%A1%E5%9E%8B/image-20210304183209619.png" alt="image-20210304183209619" style="zoom:50%;"></p>
</li>
<li><p>HMM模型做了两个很重要的假设如下:</p>
<ol>
<li><p><code>齐次马尔科夫链假设：</code>即任意时刻的隐藏状态只依赖于它前一个隐藏状态。</p>
<p>如果在时刻<em>t</em>的隐藏状态是$i_t=q_i$，在时刻<em>t</em>+1的隐藏状态是$i_{t+1}=q_j$ ，则从时刻<em>t</em>到时刻<em>t</em>+1的HMM状态转移概率$a_{ij}$ 可以表 示为:$a_{ij}=P(i_{i+1}=q_j|i_t=q_i)$<br>这样$a_{ij}$可以组成马尔科夫链的状态转移矩阵 <em>A</em> :</p>
<p><em>A</em> = [$a_{ij}$]<em>N</em>×<em>N</em></p>
</li>
<li><p><code>观测独立性假设:</code>即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态。</p>
<p>如果在时刻t的隐藏状态是$i_t=q_j$, 而对应的观察状态为 $o_t=v_k$ , 则该时刻观察状态$v_k$在隐藏状态$q_j$下生成的概率为$b_j(k)$，满足:$b_j(k)=P(o_t=v_k|i_t=q_j)$<br>这样$b_j(k)$可以组成观测状态生成的概率矩阵 <em>B</em> :</p>
<p><em>B</em> = $[b_j(k)]_{N\times M}$<br>除此之外，我们需要一组在时刻 <em>t</em> = 1 的隐藏状态概率分布 Π :</p>
<p>Π = $[\Pi_i]_N$，其中Π<em>i</em> =$P(i_1=q_i)$</p>
</li>
<li><p>一个HMM模型，可以由隐藏状态初始概率分布 Π <strong>,</strong> 状态转移概率矩阵 <em>A</em> 和观测状态概率矩阵 <em>B</em> 决定。 Π ， <em>A</em> 决定状态序列，<em>B</em> 决定观测序列。<br> 因此，HMM模型可以由一个三元组 <em>λ</em> 表示如下:</p>
<p><em>λ</em> = (<em>A</em>, <em>B</em>, Π) = (状态序列，观测序列，初始状态概率分布)</p>
</li>
</ol>
</li>
<li><p><code>模型三个基本问题：</code></p>
<ol>
<li><p><code>评估观察序列概率</code> —— 前向后向的概率计算：</p>
<p>即给定模型<em>λ</em> = (<em>A</em>,<em>B</em>,Π)和观测序列<em>O</em> = {$o_1,o_2,…o_T$}，计算在模型<em>λ</em>下某一个观测序列<em>O</em>出现的概率<em>P</em>(<em>O</em>∣<em>λ</em>)，这个问题的求解需要用到前向后向算法。</p>
</li>
<li><p><code>预测问题，也称为解码问题</code> ——维特比(<strong>Viterbi</strong>)算法:</p>
<p>即给定模型<em>λ</em> = (<em>A</em>,<em>B</em>,Π)和观测序列<em>O</em> = {$o_1,o_2,…o_T$}，求给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的维特比算法。</p>
</li>
<li><p><code>模型参数学习问题</code>—— 鲍姆<strong>-</strong>韦尔奇(<strong>Baum-Welch</strong>)算法(状态未知)：</p>
<p>即给定观测序列 <em>O</em> = {$o_1,o_2,…o_T$} ，估计模型 <em>λ</em> = (<em>A</em>, <em>B</em>, Π) 的参数，使该模型下观测序列的条件概率 <em>P</em> (<em>O</em>∣<em>λ</em>) 最大，这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法</p>
</li>
</ol>
</li>
</ol>
</blockquote>
<h1 id="前向后向算法"><a href="#前向后向算法" class="headerlink" title="前向后向算法"></a>前向后向算法</h1><blockquote>
<ol>
<li><p>前向概率：定义时刻 <em>t</em> 时隐藏状态为$q_i$ <strong>,</strong> 观测状态的序列为$o_1,o_2,…o_T$的概率为前向概率,记为：</p>
<p><img src="/2021/05/05/HMM%E6%A8%A1%E5%9E%8B/image-20210304184918482.png" alt="image-20210304184918482" style="zoom:75%;"></p>
</li>
<li><p>基于时刻 <em>t</em> 时各个隐藏状态的前向概率，再乘以对应的状态转移概率，即$\alpha_t(j)a_{ji}$就是在时刻 <em>t</em> 观测到序列$o_1,o_2,…o_T$并且时刻<em>t</em>隐藏状态为$q_j$,时刻<em>t</em>+1隐藏状态为$q_i$的概率。</p>
</li>
<li><p>$\sum_{j=1}^{N}\alpha_t(j)a_{ji}$就是在时刻 <em>t</em> 观测到$o_1,o_2,…o_T$并且时刻t+1隐藏状态为$q_i$的概率。</p>
</li>
<li><p>这样我们得到了前向概率的递推关系式如下:</p>
<p><img src="/2021/05/05/HMM%E6%A8%A1%E5%9E%8B/image-20210304185724229.png" alt="image-20210304185724229" style="zoom:60%;"></p>
</li>
<li><p>将所有隐藏状态对应的概率相加，即$\sum_{i=1}^N\alpha_T(i)$就得到了在时刻 <em>T</em> 观测序列为 $o_1,o_2,…o_T$的概率。</p>
</li>
</ol>
<p><strong>算法总结：</strong></p>
<ol>
<li>前向算法：</li>
</ol>
<p><img src="/2021/05/05/HMM%E6%A8%A1%E5%9E%8B/image-20210304185920961.png" alt="image-20210304185920961" style="zoom:70%;"></p>
<ol>
<li>后向算法：</li>
</ol>
<p><img src="/2021/05/05/HMM%E6%A8%A1%E5%9E%8B/image-20210304190109220.png" alt="image-20210304190109220" style="zoom:70%;"></p>
</blockquote>
<h1 id="维特比算法"><a href="#维特比算法" class="headerlink" title="维特比算法"></a>维特比算法</h1><blockquote>
<p><img src="/2021/05/05/HMM%E6%A8%A1%E5%9E%8B/image-20210304190258092.png" alt="image-20210304190258092"></p>
</blockquote>
<h1 id="鲍姆-韦尔奇"><a href="#鲍姆-韦尔奇" class="headerlink" title="鲍姆-韦尔奇"></a>鲍姆-韦尔奇</h1><blockquote>
<p>鲍姆-韦尔奇算法原理既然使用的就是EM算法的原理</p>
<ol>
<li>那么我们需要在 <em>E</em> 步求出联合分布$P(O,I|\lambda)$基于条件概率的$P(O,I|\hat{\lambda})$期望，其中<script type="math/tex">\hat{\lambda}</script>为当前的模型参数， 然后在 <em>M</em> 步最大化这个期望，得到更新的模型参数 <em>λ</em> 。</li>
<li><img src="/2021/05/05/HMM%E6%A8%A1%E5%9E%8B/image-20210304190552024.png" alt="image-20210304190552024" style="zoom:75%;"></li>
</ol>
</blockquote>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/05/05/HMM%E6%A8%A1%E5%9E%8B/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://qi-jinli.github.io/2021/05/05/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.jpeg">
      <meta itemprop="name" content="Qijinli">
      <meta itemprop="description" content="毕业于大连理工大学机械系，现从事人工智能开发，主要从事机器学习，计算机视觉，自然语言处理等多项领域。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="畅快的伊瓦西">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/05/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">集成学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-05 19:02:57" itemprop="dateCreated datePublished" datetime="2021-05-05T19:02:57+08:00">2021-05-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><blockquote>
<p>集成学习通过建立几个模型来解决单一预测问题。它的工作原理是<strong>生成多个分类器/模型</strong>，各自独立地学习和作出预测。<strong>这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。</strong></p>
<p><code>boosting</code>：弱弱组合变强，解决欠拟合问题，主要方法：boosting逐步增强学习</p>
<p><code>Bagging</code>：互相遏制变壮，解决过拟合问题，主要方法：Bagging采样学习集成</p>
</blockquote>
<h1 id="Bagging和随机森林"><a href="#Bagging和随机森林" class="headerlink" title="Bagging和随机森林"></a>Bagging和随机森林</h1><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><blockquote>
<p><code>Bagging集成原理：</code></p>
<ol>
<li>采样不同数据集</li>
<li>各自训练分类器</li>
<li>平权投票，获取最终结果</li>
</ol>
</blockquote>
<h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><blockquote>
<p><strong>随机森林是一个包含多个决策树的分类器</strong>，并且其输出的类别是由个别树输出的类别的众数而定。</p>
<p><strong>随机森林</strong> <strong>= Bagging +</strong> <strong>决策树</strong></p>
<p><strong>随机森林够造过程中的关键步骤</strong>(M表示特征数目)：</p>
<p> <strong>1)一次随机选出一个样本，有放回的抽样，重复N次(有可能出现重复的样本)</strong></p>
<p> <strong>2) 随机去选出m个特征, m &lt;&lt;M，建立决策树</strong></p>
<p><strong>包外估计：</strong></p>
<p>由于基分类器是构建在训练样本的自助抽样集上的，只有约 63.2％ 原样本集出现在中，而剩余的 36.8％ 的数据作为包外数据，可以用于基分类器的验证集。</p>
<p>经验证，包外估计是对集成分类器泛化误差的<strong>无偏估计.</strong></p>
<p><strong>包外估计的用途：</strong></p>
<ul>
<li>当基学习器是决策树时，可使用包外样本来辅助剪枝 ，或用于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理；</li>
<li>当基学习器是神经网络时，可使用包外样本来辅助早期停止以减小过拟合 。</li>
</ul>
</blockquote>
<h1 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h1><blockquote>
<p><strong>随着学习的积累从弱到强</strong></p>
<p><strong>简而言之：每新加入一个弱学习器，整体能力就会得到提升</strong></p>
<p>代表算法：Adaboost，GBDT，XGBoost，LightGBM</p>
</blockquote>
<h2 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h2>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/05/05/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://qi-jinli.github.io/2021/05/05/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.jpeg">
      <meta itemprop="name" content="Qijinli">
      <meta itemprop="description" content="毕业于大连理工大学机械系，现从事人工智能开发，主要从事机器学习，计算机视觉，自然语言处理等多项领域。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="畅快的伊瓦西">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/05/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">聚类算法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-05 19:02:46" itemprop="dateCreated datePublished" datetime="2021-05-05T19:02:46+08:00">2021-05-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><blockquote>
<p><strong>聚类算法</strong>：</p>
<p>一种典型的<strong>无监督</strong>学习算法，主要用于将相似的样本自动归到一个类别中。</p>
<p>在聚类算法中根据样本之间的相似性，将样本划分到不同的类别中，对于不同的相似度计算方法，会得到不同的聚类结果，常用的相似度计算方法有欧式距离法。</p>
</blockquote>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><blockquote>
<ul>
<li><code>随机设置K</code>个特征空间内的点作为初始的<code>聚类中心</code></li>
<li>对于其他每个点<code>计算到K个中心的距离</code>，未知的点选择最近的一个聚类中心点作为标记类别</li>
<li>接着对着标记的聚类中心之后，<code>重新计算出每个聚类的新中心点（平均值）</code></li>
<li>如果计算得出的<code>新中心点与原中心点一样（质心不再移动），那么结束</code>，否则重新进行第二步过程</li>
</ul>
<p><strong>注意</strong>:</p>
<ul>
<li>由于每次都要计算所有的样本与每一个质心之间的相似度，故在大规模的数据集上，K-Means算法的<code>收敛速度比较慢。</code></li>
</ul>
</blockquote>
<h1 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h1><h2 id="误差平方和（SSE"><a href="#误差平方和（SSE" class="headerlink" title="误差平方和（SSE)"></a>误差平方和（SSE)</h2><blockquote>
<p><img src="/2021/05/05/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/image-20210225181350279.png" alt="image-20210225181350279" style="zoom: 33%;"></p>
<p><code>定义：所有样本点到各自聚类中心的差方和</code></p>
</blockquote>
<h2 id="肘方法（Elbow-method）-K值确定"><a href="#肘方法（Elbow-method）-K值确定" class="headerlink" title="肘方法（Elbow method）-K值确定"></a>肘方法（Elbow method）-K值确定</h2><blockquote>
<p>（1）对于n个点的数据集，迭代计算k from 1 to n，每次聚类完成后计算每个点到其所属的簇中心的距离的平方和；</p>
<p>（2）平方和是会逐渐变小的，直到k==n时平方和为0，因为每个点都是它所在的簇中心本身。</p>
<p>（3）在这个平方和变化过程中，会出现一个拐点也即“肘”点，<strong>下降率突然变缓时即认为是最佳的k值</strong>。</p>
<p>在决定什么时候停止训练时，肘形判据同样有效，数据通常有更多的噪音，在<strong>增加分类无法带来更多回报时，我们停止增加类别</strong>。</p>
</blockquote>
<h2 id="轮廓系数法"><a href="#轮廓系数法" class="headerlink" title="轮廓系数法"></a>轮廓系数法</h2>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/05/05/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://qi-jinli.github.io/2021/05/05/%E5%86%B3%E7%AD%96%E6%A0%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.jpeg">
      <meta itemprop="name" content="Qijinli">
      <meta itemprop="description" content="毕业于大连理工大学机械系，现从事人工智能开发，主要从事机器学习，计算机视觉，自然语言处理等多项领域。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="畅快的伊瓦西">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/05/%E5%86%B3%E7%AD%96%E6%A0%91/" class="post-title-link" itemprop="url">决策树</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-05 19:02:35" itemprop="dateCreated datePublished" datetime="2021-05-05T19:02:35+08:00">2021-05-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><blockquote>
<p><strong>决策树：</strong></p>
<ul>
<li><strong>是一种树形结构，本质是一颗由多个判断节点组成的树</strong></li>
<li><strong>其中每个内部节点表示一个属性上的判断，</strong></li>
<li><strong>每个分支代表一个判断结果的输出，</strong></li>
<li><strong>最后每个叶节点代表一种分类结果</strong>。</li>
</ul>
<p><img src="/2021/05/05/%E5%86%B3%E7%AD%96%E6%A0%91/image-20210221161453845.png" alt="image-20210221161453845" style="zoom:50%;"></p>
</blockquote>
<h1 id="决策树分类原理"><a href="#决策树分类原理" class="headerlink" title="决策树分类原理"></a>决策树分类原理</h1><h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><blockquote>
<p>1、<strong>从信息的完整性上进行的描述:</strong></p>
<p>当<strong>系统的有序状态一致时</strong>，数据越集中的地方熵值越小，数据越分散的地方熵值越大。</p>
<p>2、<strong>从信息的有序性上进行的描述:</strong></p>
<p>当<strong>数据量一致时</strong>，<strong>系统越有序，熵值越低；系统越混乱或者分散，熵值越高</strong>。</p>
<p><img src="/2021/05/05/%E5%86%B3%E7%AD%96%E6%A0%91/image-20210221161756174.png" alt="image-20210221161756174"></p>
<p>$p_{k}=\frac{C^{k}}{D}$, D为样本的所有数量，$C^{k}$为第k类样本的数量。</p>
</blockquote>
<h2 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h2><blockquote>
<p><strong>信息增益：</strong>以某特征划分数据集前后的熵的差值。熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以<strong>使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏</strong>。</p>
<p><strong>信息增益 = entroy(前) - entroy(后)</strong></p>
<blockquote>
<p>注：信息增益表示得知特征X的信息而使得类Y的信息熵减少的程度</p>
</blockquote>
<p>特征a对训练数据集D的信息增益Gain(D,a),定义为<strong>集合D的信息熵Ent(D)</strong>与给定特征a条件下D的信息条件熵Ent(D|a)之差，即公式为：</p>
<p><img src="/2021/05/05/%E5%86%B3%E7%AD%96%E6%A0%91/image-20210221163011933.png" alt="image-20210221163011933" style="zoom:50%;"></p>
<p>其中：</p>
<p>$D^v$表示a属性中第v个分支节点包含的样本数</p>
<p>$C^{kv}$表示a属性中第v个分支节点包含的样本数中，第k个类别下包含的样本数</p>
<p><strong>注意：ID3 决策树学习算法就是以信息增益为准则来选择划分属性。</strong></p>
</blockquote>
<h2 id="信息增益率"><a href="#信息增益率" class="headerlink" title="信息增益率"></a>信息增益率</h2><blockquote>
<p>信息增益准则对可取值数目较多的属性有所偏好，<code>C4.5 决策树算法</code>不直接使用信息增益，而是使用<code>增益率</code> (gain ratio) 来选择最优划分属性.</p>
<p><strong>增益率：</strong></p>
<p>增益率是用前面的信息增益Gain(D, a)和属性a对应的”固有值”(intrinsic value) 的比值来共同定义的。</p>
<p><img src="/2021/05/05/%E5%86%B3%E7%AD%96%E6%A0%91/image-20210221163429928.png" alt="image-20210221163429928" style="zoom:50%;"></p>
<p><strong>优势：</strong></p>
<p><strong>1.用信息增益率来选择属性</strong></p>
<p>克服了用信息增益来选择属性时偏向选择值多的属性的不足。</p>
<p><strong>2.采用了一种后剪枝方法</strong></p>
<p>避免树的高度无节制的增长，避免过度拟合数据</p>
<p><strong>3.对于缺失值的处理</strong></p>
<p>处理缺少属性值的一种策略是赋给它结点n所对应的训练实例中该属性的最常见值；</p>
<p>另外一种更复杂的策略是为A的每个可能值赋予一个概率。</p>
</blockquote>
<h2 id="基尼值和基尼指数"><a href="#基尼值和基尼指数" class="headerlink" title="基尼值和基尼指数"></a>基尼值和基尼指数</h2>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/05/05/%E5%86%B3%E7%AD%96%E6%A0%91/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://qi-jinli.github.io/2021/05/05/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.jpeg">
      <meta itemprop="name" content="Qijinli">
      <meta itemprop="description" content="毕业于大连理工大学机械系，现从事人工智能开发，主要从事机器学习，计算机视觉，自然语言处理等多项领域。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="畅快的伊瓦西">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/05/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" class="post-title-link" itemprop="url">逻辑回归</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-05 19:02:24" itemprop="dateCreated datePublished" datetime="2021-05-05T19:02:24+08:00">2021-05-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><blockquote>
<p><strong>sigmoid函数：</strong></p>
<script type="math/tex; mode=display">
g(z)=\frac{1}{1+e^{-z}}\\
z=\omega^{T}x</script><ul>
<li>回归的结果输入到sigmoid函数当中</li>
<li>输出结果：[0, 1]区间中的一个概率值，默认为0.5为阈值</li>
</ul>
</blockquote>
<h1 id="损失及优化"><a href="#损失及优化" class="headerlink" title="损失及优化"></a>损失及优化</h1><blockquote>
<p>逻辑回归是在线性函数的基础上，经过激活函数后产生的<code>0~1</code>之间的概率值。<br>设x为特征向量，y为真实的标签，$\hat{y}$是预测值。得出：</p>
<p><img src="/2021/05/05/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20210221152143456.png" alt="image-20210221152143456" style="zoom:50%;"></p>
<p>合并可得：</p>
<p><img src="/2021/05/05/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20210221152207350.png" alt="image-20210221152207350" style="zoom:50%;"></p>
<p>最大化似然函数也就是最小化损失函数</p>
<p><code>损失函数</code>为</p>
<p><img src="/2021/05/05/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20210221152321431.png" alt="image-20210221152321431" style="zoom:50%;"></p>
<p><code>优化:</code></p>
<p>同样使用梯度下降优化算法，去减少损失函数的值。这样去更新逻辑回归前面对应算法的权重参数，<strong>提升原本属于1类别的概率，降低原本是0类别的概率。</strong></p>
<script type="math/tex; mode=display">
\theta_{j}:=\theta_{j}-\alpha\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}</script></blockquote>
<h1 id="分类评估方法"><a href="#分类评估方法" class="headerlink" title="分类评估方法"></a>分类评估方法</h1><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><blockquote>
<p><img src="/2021/05/05/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/image-20210221155107031.png" alt="image-20210221155107031" style="zoom: 25%;"></p>
</blockquote>
<h2 id="精确率与召回率"><a href="#精确率与召回率" class="headerlink" title="精确率与召回率"></a>精确率与召回率</h2><blockquote>
<p><code>精确率</code>：预测结果为正例样本中真实为正例的比例</p>
<script type="math/tex; mode=display">
P=\frac{TP}{TP+FP}</script><p><code>召回率</code>：真实为正例的样本中预测结果为正例的比例（查得全，对正样本的区分能力）</p>
<script type="math/tex; mode=display">
R=\frac{TP}{TP+FN}</script><p><code>F1-score</code>:反映了模型的稳健型</p>
<script type="math/tex; mode=display">
F1=\frac{2\times P\times R} {P+R}=\frac{2\times P\times R}{样例总数+TP-TN}\\
F_{\beta}=\frac{\left (1+\beta^{2}\right)\times P\times R} {\left (\beta ^{2}\times P\right)+R}</script><p>当$\beta$&gt;1时，查全率有更大影响；</p>
<p>当$\beta$&lt;1时，查准率有更大影响；</p>
<p>当$\beta$=1时，退化为标准的F1</p>
</blockquote>
<h2 id="ROC曲线和AUC指标"><a href="#ROC曲线和AUC指标" class="headerlink" title="ROC曲线和AUC指标"></a>ROC曲线和AUC指标</h2>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/05/05/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://qi-jinli.github.io/2021/05/05/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.jpeg">
      <meta itemprop="name" content="Qijinli">
      <meta itemprop="description" content="毕业于大连理工大学机械系，现从事人工智能开发，主要从事机器学习，计算机视觉，自然语言处理等多项领域。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="畅快的伊瓦西">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/05/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" class="post-title-link" itemprop="url">朴素贝叶斯</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-05 19:02:05" itemprop="dateCreated datePublished" datetime="2021-05-05T19:02:05+08:00">2021-05-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><blockquote>
<p>叶斯分类算法是统计学的一种分类方法，它是一类利用概率统计知识进行分类的算法。在许多场合，朴素贝叶斯(Naïve Bayes，NB)分类算法可以与决策树和神经网络分类算法相媲美，该算法能运用到大型数据库中，而且方法简单、分类准确率高、速度快。</p>
<p>由于贝叶斯定理假设一个属性值对给定类的影响独立于其它属性的值，而此假设在实际情况中经常是不成立的，因此其分类准确率可能会下降。为此，就衍生出许多降低独立性假设的贝叶斯分类算法，如TAN(tree augmented Bayes network)算法。</p>
</blockquote>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><blockquote>
<ul>
<li>联合概率：包含多个条件，且所有条件同时成立的概率<ul>
<li>记作：P(A,B)</li>
</ul>
</li>
<li>条件概率：就是事件A在另外一个事件B已经发生条件下的发生概率<ul>
<li>记作：P(A|B)</li>
</ul>
</li>
<li>相互独立：如果P(A, B) = P(A)P(B)，则称事件A与事件B相互独立。</li>
</ul>
<p><strong>贝叶斯公式：</strong></p>
<p><img src="/2021/05/05/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/image-20210225170258975.png" alt="image-20210225170258975" style="zoom:50%;"></p>
<p><strong>拉普拉斯平滑系数：</strong></p>
<p><img src="/2021/05/05/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/image-20210225170419809.png" alt="image-20210225170419809" style="zoom:50%;"></p>
<p>其中，$\alpha$一般为1，m为特征的个数</p>
</blockquote>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><blockquote>
<ul>
<li>优点：<ul>
<li>朴素贝叶斯模型发源于古典数学理论，<strong>有稳定的分类效率</strong></li>
<li>对<strong>缺失数据不太敏感</strong>，算法也比较简单，<strong>常用于文本分类</strong></li>
<li>分类准确度高，速度快</li>
</ul>
</li>
<li>缺点：<ul>
<li>由于使用了样本属性独立性的假设，所以<strong>如果特征属性有关联时其效果不好</strong></li>
<li>需要计算先验概率，而先验概率很多时候取决于假设，假设的模型可以有很多种，因此<strong>在某些时候会由于假设的先验模型的原因导致预测效果不佳</strong>；</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="效果好的原因"><a href="#效果好的原因" class="headerlink" title="效果好的原因"></a>效果好的原因</h2><blockquote>
<ul>
<li>人们在使用分类器之前，首先做的第一步（也是最重要的一步）往往是<strong>特征选择</strong>，这个过程的目的就是为了<strong>排除特征之间的共线性、选择相对较为独立的特征</strong>；</li>
<li>对于分类任务来说，<strong>只要各类别的条件概率排序正确，无需精准概率值就可以得出正确分类</strong>；</li>
<li>如果<strong>属性间依赖对所有类别影响相同，或依赖关系的影响能相互抵消，</strong>则属性条件独立性假设在降低计算复杂度的同时不会对性能产生负面影响。</li>
</ul>
</blockquote>
<h2 id="和逻辑回归的区别"><a href="#和逻辑回归的区别" class="headerlink" title="和逻辑回归的区别"></a>和逻辑回归的区别</h2>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/05/05/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://qi-jinli.github.io/2021/05/05/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.jpeg">
      <meta itemprop="name" content="Qijinli">
      <meta itemprop="description" content="毕业于大连理工大学机械系，现从事人工智能开发，主要从事机器学习，计算机视觉，自然语言处理等多项领域。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="畅快的伊瓦西">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/05/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" class="post-title-link" itemprop="url">支持向量机</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-05 19:00:28" itemprop="dateCreated datePublished" datetime="2021-05-05T19:00:28+08:00">2021-05-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><blockquote>
<p>SVM：<strong>SVM全称是supported vector machine（支持向量机），即寻找到一个超平面使样本分成两类，并且间隔最大。</strong></p>
<p>SVM能够执行线性或非线性分类、回归，甚至是异常值检测任务。它是机器学习领域最受欢迎的模型之一。SVM特别适用于中小型复杂数据集的分类。</p>
</blockquote>
<h1 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h1><blockquote>
<p><img src="/2021/05/05/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/image-20210303185005211.png" alt="image-20210303185005211" style="zoom: 33%;"></p>
<p>根据已有训练集，通过间隔最大化得到分离超平面：<em>y</em>(<em>x</em>)=$w^T$Φ(<em>x</em>)+<em>b</em></p>
<p>相应的决策函数为：<em>f</em>(<em>x</em>)=sign($w^T$Φ(<em>x</em>)+<em>b</em>),线性可分支持向量机。</p>
<p>Φ(<em>x</em>)为核函数。</p>
<p><strong>求解过程：</strong></p>
<ol>
<li><p>样本空间中任意点x到超平面（w,b）的距离可写成：</p>
<p><img src="/2021/05/05/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/image-20210303185429679.png" alt="image-20210303185429679" style="zoom: 33%;"></p>
<p>假设超平面可以正确分类，令：</p>
<p><img src="/2021/05/05/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/image-20210303185528918.png" alt="image-20210303185528918" style="zoom: 33%;"></p>
<p>两个异类支持向量到超平面的距离之和为:$\gamma=2/||w||$</p>
</li>
<li><p>间隔最大化，即为：</p>
<p><img src="/2021/05/05/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/image-20210303185729519.png" alt="image-20210303185729519" style="zoom:33%;"></p>
</li>
<li><p>拉格朗日乘子法，约束变为无约束：</p>
<p><img src="/2021/05/05/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/image-20210303185857179.png" alt="image-20210303185857179" style="zoom:33%;"></p>
</li>
<li><p>对偶问题，极小极大变为极大极小值问题：</p>
<p><img src="/2021/05/05/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/image-20210303185956543.png" alt="image-20210303185956543" style="zoom:33%;"></p>
</li>
<li><p>对原目标函数求导：</p>
<p><img src="/2021/05/05/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/image-20210303190050345.png" alt="image-20210303190050345" style="zoom:50%;"></p>
<p>然后带入原函数，获得原函数极小值：</p>
<p><img src="/2021/05/05/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/image-20210303190137239.png" alt="image-20210303190137239" style="zoom: 33%;"></p>
</li>
<li><p>然后求$max_\alpha L(w,b,\alpha)$，即</p>
<p><img src="/2021/05/05/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/image-20210303190522111.png" alt="image-20210303190522111" style="zoom:50%;"></p>
<p>转换成极小值：</p>
<p><img src="/2021/05/05/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/image-20210303190548805.png" alt="image-20210303190548805" style="zoom:50%;"></p>
</li>
<li><p>求出极值$\alpha^*$,带入计算w,b:</p>
<p><img src="/2021/05/05/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/image-20210303190650559.png" alt="image-20210303190650559" style="zoom:50%;"></p>
</li>
</ol>
</blockquote>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><blockquote>
<p><strong>SVM Hinge损失函数:</strong></p>
<script type="math/tex; mode=display">
loss = \begin{cases}
    0, \quad if: \; y_i (w^T x_i + b) \ge 1 \\
    1 - y_i (w^T x_i + b), \quad if: \; y_i (w^T x_i + b) \lt 1
    \end{cases}</script><p>可改写为：</p>
<script type="math/tex; mode=display">
loss=max(0,1−yi(wTxi+b))</script><p>为了方便计算我们令：$ξ=1−yi(w^Txi+b)，则1−ξ=yi(w^Txi+b)$</p>
<p><strong>0/1损失：</strong></p>
<ol>
<li>划分正确，损失为0</li>
<li>划分错误，损失为1</li>
</ol>
<p><strong>Logistic损失函数：</strong></p>
<ul>
<li>损失函数的公式为：$ln(1+e^{-y_i})$，为了好看除以ln2</li>
</ul>
<p>函数图像：</p>
<p><img src="/2021/05/05/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/image-20210303191802382.png" alt="image-20210303191802382" style="zoom: 33%;"></p>
</blockquote>
<h1 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h1><h2 id="核函数概念"><a href="#核函数概念" class="headerlink" title="核函数概念"></a>核函数概念</h2><blockquote>
<p><code>核函数</code>是将原始输入空间映射到新的特征空间，从而，使得原本线性不可分的样本可能在核空间可分。</p>
<ul>
<li>假设X是输入空间，</li>
<li>H是特征空间，</li>
<li>存在一个映射ϕ使得X中的点x能够计算得到H空间中的点h，</li>
<li>对于所有的X中的点都成立：$h=\phi(x)$</li>
<li>若x，z是X空间中的点，函数k(x,z)满足下述条件，那么都成立，则称k为核函数，而ϕ为映射函数：$k(x,z)=\phi(x)\phi(z)$</li>
</ul>
<p><strong>理解：</strong></p>
<p><strong>核函数为映射后高维样本的点积，而这个点积可以用原样本的坐标来表示。</strong></p>
</blockquote>
<h2 id="常见核函数"><a href="#常见核函数" class="headerlink" title="常见核函数"></a>常见核函数</h2>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/05/05/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://qi-jinli.github.io/2021/05/05/KNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.jpeg">
      <meta itemprop="name" content="Qijinli">
      <meta itemprop="description" content="毕业于大连理工大学机械系，现从事人工智能开发，主要从事机器学习，计算机视觉，自然语言处理等多项领域。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="畅快的伊瓦西">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/05/KNN/" class="post-title-link" itemprop="url">KNN</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-05 18:59:55" itemprop="dateCreated datePublished" datetime="2021-05-05T18:59:55+08:00">2021-05-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-11-19 13:14:29" itemprop="dateModified" datetime="2021-11-19T13:14:29+08:00">2021-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="KNN概念"><a href="#KNN概念" class="headerlink" title="KNN概念"></a>KNN概念</h1><blockquote>
<p><strong>定义：</strong></p>
<p>如果一个样本在特征空间中的<strong>k</strong>个最相似<strong>(</strong>即特征空间中最邻近<strong>)</strong>的样本中的大多数属于某一个类别，则该样本也属于这个类别。</p>
</blockquote>
<h1 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h1><h2 id="欧氏距离"><a href="#欧氏距离" class="headerlink" title="欧氏距离"></a>欧氏距离</h2><blockquote>
<p><img src="/2021/05/05/KNN/image-20210220214011621.png" alt="image-20210220214011621"></p>
</blockquote>
<h2 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h2><blockquote>
<p><img src="/2021/05/05/KNN/image-20210220214048481.png" alt="image-20210220214048481"></p>
</blockquote>
<h2 id="切比雪夫距离"><a href="#切比雪夫距离" class="headerlink" title="切比雪夫距离"></a>切比雪夫距离</h2><blockquote>
<p><img src="/2021/05/05/KNN/image-20210220214136794.png" alt="image-20210220214136794"></p>
</blockquote>
<h2 id="闵可夫斯基距离"><a href="#闵可夫斯基距离" class="headerlink" title="闵可夫斯基距离"></a>闵可夫斯基距离</h2>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/05/05/KNN/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://qi-jinli.github.io/2021/05/05/EM%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.jpeg">
      <meta itemprop="name" content="Qijinli">
      <meta itemprop="description" content="毕业于大连理工大学机械系，现从事人工智能开发，主要从事机器学习，计算机视觉，自然语言处理等多项领域。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="畅快的伊瓦西">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/05/EM%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">EM算法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-05 18:59:33" itemprop="dateCreated datePublished" datetime="2021-05-05T18:59:33+08:00">2021-05-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><blockquote>
<p><code>EM算法</code>也称期望最大化(Expectation-Maximum,简称EM)算法。 它是一个基础算法，是很多机器学习领域算法的基础，比如隐式马尔科夫算法(HMM)等等。 EM算法是一种迭代优化策略，由于它的计算方法中每一次迭代都分两步，</p>
<p>其中一个为期望步(<strong>E</strong>步)， 另一个为极大步(<strong>M</strong>步)，</p>
<p>所以算法被称为EM算法(Expectation-Maximization Algorithm)。</p>
</blockquote>
<h1 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h1><blockquote>
<ol>
<li><p>写出似然函数：</p>
<p><img src="/2021/05/05/EM%E7%AE%97%E6%B3%95/image-20210304131655835.png" alt="image-20210304131655835" style="zoom:50%;"></p>
</li>
<li><p>对似然函数取对数：</p>
<p><img src="/2021/05/05/EM%E7%AE%97%E6%B3%95/image-20210304131708768.png" alt="image-20210304131708768" style="zoom:50%;"></p>
</li>
<li><p><em>n</em>个样本观察数据$x=(x_1,x_2,…x_n)$，未观察到的隐含数据$z=(z_1,z_2,…z_n)$， 联合分布 <em>p</em>(<em>x</em>, <em>z</em>; <em>θ</em>) ，条件分布 <em>p</em>(<em>z</em>∣<em>x</em>, <em>θ</em>) ，最大迭代次数<em>J</em>。</p>
</li>
<li><p>随机初始化模型参数 <em>θ</em> 的初值$\theta_0$ ,<em>j</em> = 1, 2, …, <em>J</em>开始EM算法迭代:</p>
<ol>
<li><p><strong>E</strong>步:计算联合分布的条件概率期望:</p>
<p><img src="/2021/05/05/EM%E7%AE%97%E6%B3%95/image-20210304131724146.png" alt="image-20210304131724146" style="zoom:50%;"></p>
</li>
<li><p><strong>M</strong>步:极大化 $l(\theta,\theta_j)$ <strong>,</strong>得到$\theta_{j+1}$ :</p>
<p><img src="/2021/05/05/EM%E7%AE%97%E6%B3%95/image-20210304131735708.png" alt="image-20210304131735708" style="zoom:50%;"></p>
</li>
<li><p>如果$\theta_{j+1}$已经收敛，则算法结束。否则继续进行<strong>E</strong>步和<strong>M</strong>步进行迭代。</p>
</li>
</ol>
</li>
<li><p>输出:模型参数 <em>θ</em></p>
</li>
</ol>
</blockquote>
<h1 id="算法推导"><a href="#算法推导" class="headerlink" title="算法推导"></a>算法推导</h1><h2 id="Jensen不等式"><a href="#Jensen不等式" class="headerlink" title="Jensen不等式"></a>Jensen不等式</h2><blockquote>
<p>设 f 是定义域为实数的函数，如果对所有实数X，f的二阶导数恒大于等于0，那么f为凸函数。Jensen不等式表达如下：</p>
<p>如果 f 为凸函数，X为随机变量，那么：</p>
<p><img src="/2021/05/05/EM%E7%AE%97%E6%B3%95/image-20210304131800686.png" alt="image-20210304131800686" style="zoom:50%;"></p>
<p>其中E[ ] 为期望，也称为均值。</p>
<p>Jensen不等式的等号成立的条件是当X为常量，即：函数f 的值为一条直线。</p>
</blockquote>
<h2 id="概率相关"><a href="#概率相关" class="headerlink" title="概率相关"></a>概率相关</h2><blockquote>
<p>若随机变量X的分布用分布列 p(xi)或用密度函数 p(x)表示，则X的某一函数的<strong>数学期望</strong>为：</p>
<p><img src="/2021/05/05/EM%E7%AE%97%E6%B3%95/image-20210304131859562.png" alt="image-20210304131859562" style="zoom: 33%;"></p>
<p><strong>边缘分布列：</strong>在二维离散随机变量(X, Y)的联合分布列{P(X=xi, Y=yj)}中，对j求和所得的分布列</p>
<p><img src="/2021/05/05/EM%E7%AE%97%E6%B3%95/image-20210304131925477.png" alt="image-20210304131925477" style="zoom:50%;"></p>
</blockquote>
<h2 id="EM算法推导"><a href="#EM算法推导" class="headerlink" title="EM算法推导"></a>EM算法推导</h2>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/05/05/EM%E7%AE%97%E6%B3%95/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://qi-jinli.github.io/2021/05/05/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.jpeg">
      <meta itemprop="name" content="Qijinli">
      <meta itemprop="description" content="毕业于大连理工大学机械系，现从事人工智能开发，主要从事机器学习，计算机视觉，自然语言处理等多项领域。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="畅快的伊瓦西">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/05/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" class="post-title-link" itemprop="url">线性回归</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-05 01:08:43" itemprop="dateCreated datePublished" datetime="2021-05-05T01:08:43+08:00">2021-05-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><blockquote>
<p><code>线性回归(Linear regression)</code>是利用<strong>回归方程(函数)</strong>对<strong>一个或多个自变量(特征值)和因变量(目标值)之间</strong>关系进行建模的一种分析方式。</p>
<p><code>特点</code>：只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归</p>
<p><img src="/2021/05/05/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20210221102904866.png" alt="image-20210221102904866"></p>
<p>非线性关系回归方程可以理解为：<script type="math/tex">\omega_1x_1+\omega_2x_2^2+\omega_3x_3^3</script></p>
</blockquote>
<h1 id="损失及优化"><a href="#损失及优化" class="headerlink" title="损失及优化"></a>损失及优化</h1><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><blockquote>
<p><img src="/2021/05/05/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20210221103604185.png" alt="image-20210221103604185" style="zoom:50%;"></p>
</blockquote>
<h2 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h2><h3 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h3><blockquote>
<p><img src="/2021/05/05/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20210221103502714.png" alt="image-20210221103502714" style="zoom: 33%;"></p>
<p>推导过程：</p>
<p><img src="/2021/05/05/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20210221103627823.png" alt="image-20210221103627823" style="zoom:50%;"></p>
<p><strong>求导公式：</strong></p>
<p><img src="/2021/05/05/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20210221103732267.png" alt="image-20210221103732267" style="zoom: 33%;"></p>
</blockquote>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><blockquote>
<p><strong>梯度：</strong></p>
<ul>
<li><strong>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率；</strong></li>
<li><strong>在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向</strong></li>
</ul>
<p><strong>损失函数</strong></p>
<script type="math/tex; mode=display">
J(\omega)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2</script><p><strong>梯度下降推导：</strong></p>
<p><img src="/2021/05/05/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20210221110819120.png" alt="image-20210221110819120" style="zoom:50%;"></p>
<p><strong>梯度下降公式：</strong></p>
<script type="math/tex; mode=display">
\theta_{j}:=\theta_{j}-\frac{\alpha}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}</script></blockquote>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/05/05/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Qijinli"
      src="/images/logo.jpeg">
  <p class="site-author-name" itemprop="name">Qijinli</p>
  <div class="site-description" itemprop="description">毕业于大连理工大学机械系，现从事人工智能开发，主要从事机器学习，计算机视觉，自然语言处理等多项领域。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">27</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qi Jinli</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
