<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo.jpeg"/>
	<link rel="shortcut icon" href="/img/logo.jpeg">
	
			    <title>
    Qijinli's Blog
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="qijinli" />
    
    	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: 'portrait';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover;
        }
    </style>

			    
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<meta name="generator" content="Hexo 5.4.0"></head>
    <div id="music">
    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=467952048&auto=1&height=66"></iframe>
</div>

<style>
    #music {
        position: fixed;
        right: 15px;
        bottom: 0;
        z-index: 998;
    }
</style>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<link rel="stylesheet" href="/css/toc.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">QIJINLI</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">主页</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">分类</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="../categories/Linux/">Linux</a></li><li><a class="category-link" href="../categories/python/">python</a></li><li><a class="category-link" href="../categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a></li><li><a class="category-link" href="../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li><a class="category-link" href="../categories/%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E5%BA%93/">科学计算库</a>
	                    </ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="简历">
		                简历
		            </a>
		        </li>
		        
		        <li>
		            <a href="/gallery/" title="图库">
		                图库
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="标签">
		                标签
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="../https:/github.com/Qi-jinli" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(/images/img/线性回归.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >线性回归</h2></a>
            </div>
            <div class="al_page_content_outline">
                <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5"><span class="toc-number">1.</span> <span class="toc-text">概念</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%8F%8A%E4%BC%98%E5%8C%96"><span class="toc-number">2.</span> <span class="toc-text">损失及优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">2.2.</span> <span class="toc-text">优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B"><span class="toc-number">2.2.1.</span> <span class="toc-text">正规方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.2.2.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%EF%BC%88FG"><span class="toc-number">2.2.2.1.</span> <span class="toc-text">全梯度下降算法（FG)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%EF%BC%88SG%EF%BC%89"><span class="toc-number">2.2.2.2.</span> <span class="toc-text">随机梯度下降算法（SG）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%EF%BC%88mini-batch%EF%BC%89"><span class="toc-number">2.2.2.3.</span> <span class="toc-text">小批量梯度下降算法（mini-batch）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E5%B9%B3%E5%9D%87%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%EF%BC%88SAG%EF%BC%89"><span class="toc-number">2.2.2.4.</span> <span class="toc-text">随机平均梯度下降算法（SAG）</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">3.</span> <span class="toc-text">欠拟合和过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">3.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E5%9B%A0%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">原因及解决办法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">3.3.</span> <span class="toc-text">正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Ridge-Regression%EF%BC%88%E5%B2%AD%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="toc-number">3.3.1.</span> <span class="toc-text">Ridge Regression（岭回归）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lasso-Regression%EF%BC%88Lasso%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="toc-number">3.3.2.</span> <span class="toc-text">Lasso Regression（Lasso回归）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Elastic-Net%EF%BC%88%E5%BC%B9%E6%80%A7%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="toc-number">3.3.3.</span> <span class="toc-text">Elastic Net（弹性网络）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">3.3.4.</span> <span class="toc-text">小结</span></a></li></ol></li></ol></li></ol>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><blockquote>
<p><code>线性回归(Linear regression)</code>是利用<strong>回归方程(函数)</strong>对<strong>一个或多个自变量(特征值)和因变量(目标值)之间</strong>关系进行建模的一种分析方式。</p>
<p><code>特点</code>：只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归</p>
<p><img src="/2021/02/21/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20210221102904866.png" alt="image-20210221102904866"></p>
<p>非线性关系回归方程可以理解为：<script type="math/tex">\omega_1x_1+\omega_2x_2^2+\omega_3x_3^3</script></p>
</blockquote>
<h1 id="损失及优化"><a href="#损失及优化" class="headerlink" title="损失及优化"></a>损失及优化</h1><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><blockquote>
<p><img src="/2021/02/21/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20210221103604185.png" alt="image-20210221103604185" style="zoom:50%;"></p>
</blockquote>
<h2 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h2><h3 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h3><blockquote>
<p><img src="/2021/02/21/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20210221103502714.png" alt="image-20210221103502714" style="zoom: 33%;"></p>
<p>推导过程：</p>
<p><img src="/2021/02/21/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20210221103627823.png" alt="image-20210221103627823" style="zoom:50%;"></p>
<p><strong>求导公式：</strong></p>
<p><img src="/2021/02/21/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20210221103732267.png" alt="image-20210221103732267" style="zoom: 33%;"></p>
</blockquote>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><blockquote>
<p><strong>梯度：</strong></p>
<ul>
<li><strong>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率；</strong></li>
<li><strong>在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向</strong></li>
</ul>
<p><strong>损失函数</strong></p>
<script type="math/tex; mode=display">
J(\omega)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2</script><p><strong>梯度下降推导：</strong></p>
<p><img src="/2021/02/21/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20210221110819120.png" alt="image-20210221110819120" style="zoom:50%;"></p>
<p><strong>梯度下降公式：</strong></p>
<script type="math/tex; mode=display">
\theta_{j}:=\theta_{j}-\frac{\alpha}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}</script></blockquote>
<h4 id="全梯度下降算法（FG"><a href="#全梯度下降算法（FG" class="headerlink" title="全梯度下降算法（FG)"></a>全梯度下降算法（FG)</h4><blockquote>
<p>批量梯度下降法，是梯度下降法最常用的形式，<strong>具体做法也就是在更新参数时使用所有的样本来进行更新。</strong></p>
<p><strong>计算训练集所有样本误差</strong>，<strong>对其求和再取平均值作为目标函数</strong>。</p>
<script type="math/tex; mode=display">
\theta_{j}:=\theta_{j}-\alpha\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}</script></blockquote>
<h4 id="随机梯度下降算法（SG）"><a href="#随机梯度下降算法（SG）" class="headerlink" title="随机梯度下降算法（SG）"></a>随机梯度下降算法（SG）</h4><blockquote>
<p><strong>每次只代入计算一个样本目标函数的梯度来更新权重，再取下一个样本重复此过程，直到损失函数值停止下降或损失函数值小于某个可以容忍的阈值。</strong></p>
<script type="math/tex; mode=display">
\theta_{j}:=\theta_{j}-\alpha(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}</script></blockquote>
<h4 id="小批量梯度下降算法（mini-batch）"><a href="#小批量梯度下降算法（mini-batch）" class="headerlink" title="小批量梯度下降算法（mini-batch）"></a>小批量梯度下降算法（mini-batch）</h4><blockquote>
<p><strong>每次从训练样本集上随机抽取一个小样本集，在抽出来的小样本集上采用FG迭代更新权重。</strong></p>
<script type="math/tex; mode=display">
\theta_{j}:=\theta_{j}-\alpha\sum_{i=t}^{t+x-1}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}</script><p><strong>被抽出的小样本集所含样本点的个数称为batch_size，通常设置为2的幂次方，更有利于GPU加速处理。</strong></p>
<p><strong>特别的，若batch_size=1，则变成了SG；若batch_size=n，则变成了FG.</strong></p>
</blockquote>
<h4 id="随机平均梯度下降算法（SAG）"><a href="#随机平均梯度下降算法（SAG）" class="headerlink" title="随机平均梯度下降算法（SAG）"></a>随机平均梯度下降算法（SAG）</h4><blockquote>
<p><strong>在内存中为每一个样本都维护一个旧的梯度，随机选择第i个样本来更新此样本的梯度，其他样本的梯度保持不变，然后求得所有梯度的平均值，进而更新了参数。</strong></p>
<script type="math/tex; mode=display">
\theta_{j}:=\theta_{j}-\frac{\alpha}{n}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}</script></blockquote>
<h1 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><blockquote>
<ul>
<li>过拟合：一个假设<strong>在训练数据上能够获得比其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据</strong>，此时认为这个假设出现了过拟合的现象。(模型过于复杂)</li>
<li>欠拟合：一个假设<strong>在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据</strong>，此时认为这个假设出现了欠拟合的现象。(模型过于简单)</li>
</ul>
<p><img src="/2021/02/21/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20210221114925438.png" alt="image-20210221114925438" style="zoom: 33%;"></p>
</blockquote>
<h2 id="原因及解决办法"><a href="#原因及解决办法" class="headerlink" title="原因及解决办法"></a>原因及解决办法</h2><blockquote>
<ul>
<li>欠拟合原因以及解决办法<ul>
<li>原因：学习到数据的特征过少</li>
<li>解决办法：<ul>
<li><strong>添加其他特征项，</strong>有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。</li>
<li><strong>添加多项式特征</strong>，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。</li>
</ul>
</li>
</ul>
</li>
<li>过拟合原因以及解决办法<ul>
<li>原因：原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点</li>
<li>解决办法：<ul>
<li>1）重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。</li>
<li>2）增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。</li>
<li><strong>3）正则化</strong></li>
<li>4）减少特征维度，防止<strong>维灾难</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><blockquote>
<p><strong>在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化</strong></p>
<p><strong>分类:</strong></p>
<ul>
<li>L2正则化<ul>
<li>作用：可以使得其中一些W的都很小，都接近于0，削弱某个特征的影响</li>
<li>优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象</li>
<li>Ridge回归</li>
</ul>
</li>
<li>L1正则化<ul>
<li>作用：可以使得其中一些W的值直接为0，删除这个特征的影响</li>
<li>LASSO回归</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="Ridge-Regression（岭回归）"><a href="#Ridge-Regression（岭回归）" class="headerlink" title="Ridge Regression（岭回归）"></a>Ridge Regression（岭回归）</h3><blockquote>
<p><img src="/2021/02/21/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20210221115445087.png" alt="image-20210221115445087" style="zoom:50%;"></p>
<p><strong>注意：α=0，岭回归退化为线性回归</strong></p>
</blockquote>
<h3 id="Lasso-Regression（Lasso回归）"><a href="#Lasso-Regression（Lasso回归）" class="headerlink" title="Lasso Regression（Lasso回归）"></a>Lasso Regression（Lasso回归）</h3><blockquote>
<p><img src="/2021/02/21/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20210221115645244.png" alt="image-20210221115645244" style="zoom:50%;"></p>
<p><strong>Lasso Regression 有一个很重要的性质是：倾向于完全消除不重要的权重。</strong></p>
</blockquote>
<h3 id="Elastic-Net（弹性网络）"><a href="#Elastic-Net（弹性网络）" class="headerlink" title="Elastic Net（弹性网络）"></a>Elastic Net（弹性网络）</h3><blockquote>
<p><code>弹性网络</code>在岭回归和Lasso回归中进行了折中，通过 <strong>混合比(mix ratio) r</strong> 进行控制：</p>
<ul>
<li><code>r=0</code>：弹性网络变为岭回归</li>
<li><code>r=1</code>：弹性网络便为Lasso回归</li>
</ul>
<p>弹性网络的<code>代价函数</code> ：</p>
<p><img src="/2021/02/21/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20210221115855169.png" alt="image-20210221115855169" style="zoom:50%;"></p>
</blockquote>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><blockquote>
<ul>
<li><code>Ridge Regression 岭回归</code><ul>
<li>就是把系数添加平方项</li>
<li>然后限制系数值的大小</li>
<li>α值越小，系数值越大，α越大，系数值越小</li>
</ul>
</li>
<li><code>Lasso 回归</code><ul>
<li>对系数值进行绝对值处理</li>
<li>由于绝对值在顶点处不可导，所以进行计算的过程中产生很多0，最后得到结果为：稀疏矩阵</li>
</ul>
</li>
<li><code>Elastic Net 弹性网络</code><ul>
<li>是前两个内容的综合</li>
<li>设置了一个r,如果r=0—岭回归；r=1—Lasso回归</li>
</ul>
</li>
<li><code>Early stopping</code><ul>
<li>通过限制错误率的阈值，进行停止</li>
</ul>
</li>
</ul>
</blockquote>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'http-miccall-tech'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'http://qi-jinli.github.io/2021/02/21/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://qi-jinli.github.io/2021/02/21/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//http-miccall-tech.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a target="_blank" rel="noopener" href="http://www.qijinli.work " style="border-bottom: none;">Qijinli</a></li>
            </ul>
            
                <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div>
</body>



 	
</html>
